# Introduction des paramètres après les tests faits dans le notebook

# Paramètres de split des données
data_ingestion:
  test_size: 0.20

preprocess:
  text_col: "text"
  out_col: "text_norm"
  min_words: 3


# features ingenering
feature_engineering:
  text_col: "text_norm"
  label_col: "label"
  methods: ["bow", "tfidf"]     # ou ["tfidf"] seulement
  min_df: 5 # Un mot doit apparaître dans au moins 5 documents pour être gardé
  max_df: 0.9 # Un mot qui apparaît dans plus de 90 % des documents est supprimé
  token_pattern: "\\b[a-z]{3,15}\\b" # Un token valide doit respecter exactement ce pattern regex
  ngram_range: [1, 2]
  sublinear_tf: true
  norm: "l2"



# Paramètres du modèle
model_building:
  ngram_range: [1, 2]
  max_features: 84600
  learning_rate: 0.2
  n_estimators: 800

